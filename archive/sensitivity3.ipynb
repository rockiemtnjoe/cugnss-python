{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9987c2-58a9-494f-a799-a2162ff1b3b5",
   "metadata": {},
   "source": [
    "#This notebook attempts to speed up computations from earlier sensitivity notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c66778c-1130-4202-b53c-de6eb49ddb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, time\n",
    "from typing import Tuple\n",
    "import math\n",
    "from scipy.special import erfcinv\n",
    "from scipy.stats import chi2\n",
    "from Include.preRun import pre_run\n",
    "from Include.makeCaTable import make_ca_table\n",
    "from init_settings import init_settings\n",
    "from Include.plotAcqSearch import plotAcqSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749df8bb-070f-4d3c-8569-d4f2690ff5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Read datafile, return a complex numpy vector\n",
    "#  \n",
    "\n",
    "def readAcqData(settings, code_periods = None, skip = None, framing = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    read a datafile\n",
    "    settings is the standard settings object.  We will use:\n",
    "        fileName\n",
    "        skipNumberOfBytes\n",
    "        samplingFreq\n",
    "        codeFreqBasis\n",
    "        codeLength\n",
    "        acqNonCohTime\n",
    "        acqCoherentInt\n",
    "    code_periods is the number of 1mS code periods we need\n",
    "    skip is the number of samples in the datafile to skip\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        fid = open(settings.fileName, 'rb')\n",
    "    except Exception as e:\n",
    "        # Error while opening the data file.\n",
    "        raise RuntimeError(f\"Unable to read file {settings.fileName}: {e}\")\n",
    "    \n",
    "    # Initialize the multiplier to adjust for the data type\n",
    "    data_adapt_coeff = 1 if settings.fileType == 1 else 2\n",
    "    \n",
    "    # Move the starting point of processing. Can be used to start the\n",
    "    # signal processing at any point in the data record (e.g. good for long\n",
    "    # records or for signal processing in blocks).\n",
    "    if skip == None:\n",
    "        fid.seek(data_adapt_coeff * settings.skipNumberOfBytes, 0)\n",
    "    else:\n",
    "        fid.seek(data_adapt_coeff * skip, 0)\n",
    "    \n",
    "    # %% Acquisition ============================================================\n",
    "    samples_per_code = int(round(settings.codeLength * settings.samplingFreq / settings.codeFreqBasis))\n",
    "    # At least 42ms of signal are needed for fine frequency estimation\n",
    "\n",
    "    code_len = (2*settings.acqCoherentInt)*(settings.acqNonCohTime)\n",
    "    #code_len = max(42, settings.acqNonCohTime + 2)\n",
    "    \n",
    "    if code_periods == None:\n",
    "        num_samples = data_adapt_coeff * code_len * samples_per_code\n",
    "    else:\n",
    "        num_samples = code_periods * code_len * samples_per_code\n",
    "        \n",
    "    # Read data for acquisition.\n",
    "    if settings.dataType == 'schar':\n",
    "        dtype = np.int8\n",
    "    elif settings.dataType == 'short':\n",
    "        dtype = np.int16\n",
    "    elif settings.dataType == 'float':\n",
    "        dtype = np.float32\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataType: {settings.dataType}\")\n",
    "    data = np.fromfile(fid, dtype=dtype, count=num_samples)\n",
    "    if data.size < num_samples:\n",
    "        raise ValueError('Could not read enough data from the data file.')\n",
    "    \n",
    "    if data_adapt_coeff == 2:\n",
    "        # For complex data, separate I and Q\n",
    "        data_i = data[::2]\n",
    "        data_q = data[1::2]\n",
    "        data = data_i + 1j * data_q\n",
    "    fid.close()\n",
    "    # If the framing flag is set to be true, we fill 1mS of data down each column\n",
    "    # and there is one column for each code period\n",
    "    #\n",
    "    if framing == True:\n",
    "        data = data.reshape(-1, code_len)\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d661e866-7401-42b8-89f0-9cf25e522300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data File: /mnt/e/gnss_data/L1_IF20KHz_FS18MHz/L1_IF20KHz_FS18MHz.bin\n"
     ]
    }
   ],
   "source": [
    "settings = init_settings()\n",
    "print (f'Data File: {settings.fileName}')\n",
    "longdata = readAcqData(settings, framing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0d69d2-97b0-4675-bf9e-5069f511dbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200000,)\n"
     ]
    }
   ],
   "source": [
    "print (longdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1004bdff-475c-49e9-ad24-04b8670cf0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000,)\n"
     ]
    }
   ],
   "source": [
    "PRN = 1\n",
    "thisCode = make_ca_table(PRN, settings)\n",
    "print (thisCode.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d808b08-10e4-4e83-aba1-f8f660ee1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fft as sfft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f7d0098-55ca-4396-b84e-0c0a8866eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectCASat (PRN, longData, settings, showStatus=False, nCoherent = None, nNonCoherent = None, pfa = None) -> dict:\n",
    "    \"\"\"\n",
    "    Attempt to detect the CA Code from the satellite with PRN\n",
    "    Receive a long data record from the file; this will be a single, long vector\n",
    "    Characteristics of the data and processing are provided in settings\n",
    "    \"\"\"\n",
    "    startTime = time.time()\n",
    "    if (showStatus==True):\n",
    "        print (\"Beginning Detection of PRN {PRN}\")\n",
    "\n",
    "    tSample = 1 / settings.samplingFreq\n",
    "    low_freq = -1 * settings.acqSearchBand\n",
    "    high_freq = settings.acqSearchBand\n",
    "\n",
    "    if nCoherent == None:\n",
    "        nCoherent = int(settings.acqCoherentInt)\n",
    "\n",
    "    if nNonCoherent == None:\n",
    "        nNonCoherent = int(settings.acqNonCohTime)\n",
    "    if showStatus == True:\n",
    "        print (f'Coherent Integrations: {nCoherent}, Non-Coherent Integrations: {nNonCoherent}')\n",
    "    if pfa == None:\n",
    "        pfa = 0.05  # This is the Probability of False Alarm for a satellite\n",
    "    \n",
    "    #\n",
    "    # Compute the number of samples in each code, and the number of codes\n",
    "    # that will be required\n",
    "    samples_per_code = int(round(settings.codeLength * settings.samplingFreq / settings.codeFreqBasis))\n",
    "    #print (f'samples per code: {samples_per_code}')\n",
    "    numCodes = 2*nCoherent*nNonCoherent\n",
    "    totalSamples = samples_per_code*numCodes\n",
    "    #\n",
    "    # Make sure we've got enough data to do the required analysis\n",
    "    #\n",
    "    if len(longData) < totalSamples:\n",
    "        print ('Insufficient Data to perform acquisition')\n",
    "        print (f'{totalSamples} Samples Needed, received {len(longData)}')\n",
    "        return {\"success\": False, \"Error Message\": 'Insufficient Data'}\n",
    "        # In principal, we could read the required data from the file rather than exiting\n",
    "    \n",
    "    #\n",
    "    # Remove any residual DC carrier from the input signal\n",
    "    # This is vital because we will use a chi-squared distribution later\n",
    "    # Scale so ammplitude so either real or imaginary part has max\n",
    "    # amplitude of 1.000; longData becomes a vector with abs(signal) <= 1.0\n",
    "    #\n",
    "    rl = np.real(longData)\n",
    "    im = np.imag(longData)\n",
    "    rl = rl - np.mean(rl)\n",
    "    im = im - np.mean(im)\n",
    "    max_sample = np.max(np.concatenate((rl, im)))\n",
    "    longData = ((0.5 / max_sample) * (rl + (1j * im))).astype(np.complex64)\n",
    "    #\n",
    "    # Mix longData down to baseband (nominally zero Hz IF)\n",
    "    if settings.IF != 0.0:\n",
    "        phase_points = (np.arange(len(longData)) * 2 * np.pi * tSample * settings.IF).astype(np.complex64)\n",
    "        loCarrier = np.exp(-1j * phase_points, dtype=np.complex64)\n",
    "        longData = longData * loCarrier\n",
    "    #\n",
    "    # Make a test template for the PRN we're trying to detect\n",
    "    # Make templates in both the time and frequency domain\n",
    "    # Compute the frequency bins of the fft\n",
    "    \n",
    "    pnCodeSamples = make_ca_table(PRN, settings)\n",
    "    #print (type(pnCodeSamples), pnCodeSamples.shape)\n",
    "    #print (f'first several pnChips: {pnCodeSamples[:20]}')\n",
    "    zero_padding = np.zeros(samples_per_code*nCoherent)\n",
    "    #print (f'Zero Padding: {type(zero_padding)}, {zero_padding.shape}')\n",
    "    prnTemplateTD = np.concatenate((np.tile(pnCodeSamples,nCoherent), zero_padding), dtype=np.complex64)\n",
    "    #print (f'prnTemplateTD: {prnTemplateTD.shape}, {type(prnTemplateTD)}')\n",
    "    \n",
    "    #prnTemplateFD = np.conj(np.fft.fft(prnTemplateTD), dtype=np.complex64)\n",
    "    prnTemplateFD = np.conj(sfft.fft(prnTemplateTD))\n",
    "    #\n",
    "    # Determine which frequency bins are in range for possible doppler\n",
    "    #\n",
    "    #print (f'prnTemplateFD shape: {prnTemplateFD.shape}')\n",
    "    freqs = np.fft.fftfreq(len(prnTemplateTD), tSample)\n",
    "    #print (f'frequency table: {freqs[:16]}')\n",
    "    freq_mask = (freqs >= low_freq) & (freqs <= high_freq)   # A boolean vector of frequency values in range\n",
    "    nFreqBins = np.sum(freq_mask)                            # Number of elements that are between lower and upper frequency\n",
    "    #\n",
    "    # Since we know the signal is at baseband of zero IF, \n",
    "    # we also know that half the frequencies are on either side of zero\n",
    "    initialShift = int((nFreqBins - 1) / 2)\n",
    "    #print (f'Initial Shift = {initialShift}, value = {freqs[initialShift]}')\n",
    "    freq_vals = freqs[freq_mask]                             # The values of the frequencies in bounds\n",
    "    #freq_idx = np.where(freq_mask)  #The indexes of the values that are in bounds\n",
    "    #print(f'frequencies: {freq_vals} \\n, indices: {freq_idx}')\n",
    "    #\n",
    "    # Reformat the data so there is one row vector for each \n",
    "    # Coherent Integration period.\n",
    "    # Each row will have (2*samples_per_code*nCoherent) samples\n",
    "    # It makes it a little cleaner to analyze\n",
    "    #\n",
    "    data = (longData[:totalSamples]).reshape((nNonCoherent,2*samples_per_code*nCoherent))\n",
    "    #print (data.shape)\n",
    "    #\n",
    "    # This is the main loop where we convolve the PRN sequence against the possible \n",
    "    # Frequency shifts\n",
    "    #\n",
    "    # Rows for doppler frequencies, time steps along columns\n",
    "    detector = (np.zeros((nFreqBins, samples_per_code*nCoherent*2))).astype(np.float32)\n",
    "    cohData = (np.zeros((samples_per_code*nCoherent*2))).astype(np.complex64)\n",
    "    FreqDom = (np.zeros((nFreqBins, samples_per_code*nCoherent*2))).astype(np.complex64)\n",
    "    for nonCohIndex in range (nNonCoherent):\n",
    "        if showStatus == True:\n",
    "            print (\"Noncoherent Integration {nonCohIndex}\")\n",
    "        #\n",
    "        cohDetector = np.zeros((nFreqBins, 2*samples_per_code*nCoherent), dtype=np.complex64)\n",
    "        #print (f'Looping through: {nonCohIndex}, {startCode}')\n",
    "        thisDataFD = sfft.fft(cohData)   #Vector of signal data, nCoherent codes long\n",
    "        thisDataFD = np.roll(thisDataFD, initialShift)\n",
    "        # Loop through frequencies\n",
    "        this_freq = np.roll(freqs, initialShift)\n",
    "        for i in range(nFreqBins):\n",
    "            FreqDom[i,:] = thisDataFD\n",
    "            thisDataFD = np.roll(thisDataFD, -1)\n",
    "        convolutionFD = FreqDom * np.broadcast_to(prnTemplateFD, (nFreqBins, 2*samples_per_code*nCoherent))\n",
    "        cohDetector = sfft.ifft(convolutionFD, axis=1)\n",
    "        detector += (abs(cohDetector) ** 2).astype(np.float32)  #square because for non-coherent integration we're averaging power\n",
    "    #\n",
    "    # Last step is to normalize the detector output based on sample rate and \n",
    "    # Number of non coherent detections\n",
    "    detector = ((1/nNonCoherent) * (1 / samples_per_code) * detector).astype(np.float32)\n",
    "    peak_val = np.max(detector)\n",
    "    #\n",
    "    # This is an attempt to estimate the mean under no-signal conditions (H0 condition)\n",
    "    # Assume signal is present in only a few cells, and most cells are noise.\n",
    "    # Average across rows - most rows will be noise, and they'll be pretty consistent.  \n",
    "    # If we take the median of those cells, it should give us a good estimate of median under\n",
    "    # No-signal conditions.\n",
    "    row_mean = np.mean(detector, axis = 1)\n",
    "    meanval = np.median(row_mean)\n",
    "    sigma_sq = meanval / nCoherent\n",
    "    # Compute the detection threshold\n",
    "    #\n",
    "    # We want pfa to reflect the possibility of an error in the dataset we were given\n",
    "    # there are len(detector) samples that are mostly noise-like;\n",
    "    adjpfa = pfa / len(detector)\n",
    "    gamma = math.sqrt(2) * chi2.isf(adjpfa, 2*nCoherent)\n",
    "    threshold = sigma_sq * gamma\n",
    "    \n",
    "    signalDetected = (peak_val > threshold)\n",
    "    peakIndexFlat = np.argmax(detector)\n",
    "    peakIndex = np.unravel_index(peakIndexFlat, detector.shape)\n",
    "    \n",
    "    endTime = time.time()\n",
    "    runtime = endTime - startTime\n",
    "    return {\n",
    "        \"success\":True,\n",
    "        \"signalDetected\": signalDetected,\n",
    "        \"detector\":detector,\n",
    "        \"maxValue\": peak_val,\n",
    "        \"mean\": meanval,\n",
    "        \"pfa\": pfa,\n",
    "        \"threshold\": threshold,\n",
    "        \"index\": peakIndex,\n",
    "        \"peakfrequency\": freqs[peakIndex[0]],\n",
    "        \"peaksample\": peakIndex[1],\n",
    "        \"freqtable\": np.roll(freqs, initialShift)[:nFreqBins],\n",
    "        \"runtime\": runtime\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b3a7fc-a5a4-44d0-9339-4bf42a492c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 3.8494935035705566 seconds\n",
      "Signal Detected: False\n",
      "max_location: (np.int64(0), np.int64(0))\n",
      "Maximum Correlation Value: 0.0\n",
      "Threshold Value: 0.0\n",
      "Peak / Threshold: nan\n",
      "Mean Value: 0.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_799959/732423169.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  print (f'Peak / Threshold: {maxVal / threshold}')\n"
     ]
    }
   ],
   "source": [
    "#detectionResults = detectCASat(7, longdata, settings, nCoherent = 1, nNonCoherent = 1)\n",
    "detectionResults = detectCASat(15, longdata, settings, nCoherent = 5, nNonCoherent = 10, pfa=0.01)\n",
    "#print (detectionResults)\n",
    "runtime = detectionResults[\"runtime\"]\n",
    "print (f'Completed in {runtime} seconds')\n",
    "max_location = detectionResults[\"index\"]\n",
    "detectArray = detectionResults[\"detector\"]\n",
    "sig = detectionResults[\"signalDetected\"]\n",
    "mean = detectionResults[\"mean\"]\n",
    "threshold = detectionResults[\"threshold\"]\n",
    "maxVal = detectionResults[\"maxValue\"]\n",
    "\n",
    "\n",
    "print (f'Signal Detected: {sig}')\n",
    "print (f'max_location: {max_location}')\n",
    "print (f'Maximum Correlation Value: {maxVal}')\n",
    "print (f'Threshold Value: {threshold}')\n",
    "print (f'Peak / Threshold: {maxVal / threshold}')\n",
    "print (f'Mean Value: {mean:.2e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5c0007-0516-49e0-89e6-8191683ea110",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAcqSearch(215, settings, detectArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0410ae-df8f-4e5d-b9f0-b183b1ea7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "nCoherent = 5\n",
    "nNonCoherent = 5\n",
    "print (detectArray.size)\n",
    "#variance = np.var(detectArray)\n",
    "# This is an attempt to estimate the mean under no-signal conditions\n",
    "# Assume signal is present in only a few cells, and most cells are noise.\n",
    "# Average across rows - most rows will be noise, and they'll be pretty consistent.  \n",
    "# If we take the median of those cells, it should give us a good estimate of median under\n",
    "# No-signal conditions.\n",
    "row_mean = np.mean(detectArray, axis = 1)\n",
    "meanval = np.median(row_mean)\n",
    "sigma_sq = meanval / nCoherent\n",
    "medianval = np.median(detectArray)\n",
    "maxval = np.max(detectArray)\n",
    "pfa = 0.001 * (1/detectArray.size)\n",
    "gamma = math.sqrt(2) * chi2.isf(pfa, 2*nCoherent)\n",
    "print (f' max = {maxval}\\n mean= {meanval}\\n median= {medianval}\\n variance= {variance} \\n pfa = {pfa} \\n gamma = {gamma}')\n",
    "threshold = (sigma_sq / nNonCoherent) * gamma\n",
    "print (f'Threshold = {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf860d27-ca96-4515-b11a-310d51f44981",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAcqSearch(99, settings, detectArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120ede5-3933-42c0-a6e8-a1137cc0961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nCoherent = 5\n",
    "nNonCoherent = 5\n",
    "pFA = 0.001\n",
    "\n",
    "sv23_Results = detectCASat(23, longdata, settings, nCoherent = nCoherent, nNonCoherent = nNonCoherent, pfa=pFA)\n",
    "\n",
    "detectArray = sv23_Results[\"detector\"]\n",
    "signalDetected = sv23_Results[\"signalDetected\"]\n",
    "maxValue = sv23_Results[\"maxValue\"]\n",
    "threshold = sv23_Results[\"threshold\"]\n",
    "print (f'Signal Detected = {signalDetected}')\n",
    "print (f'Max Peak = {maxValue}')\n",
    "print (f'Threshold Value = {threshold}')\n",
    "print (f'DetectArray shape = {detectArray.shape}')\n",
    "#variance = np.var(detectArray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc987669-7a07-434c-824c-ccef101ac930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an attempt to estimate the mean under no-signal conditions (H0 condition)\n",
    "# Assume signal is present in only a few cells, and most cells are noise.\n",
    "# Average across rows - most rows will be noise, and they'll be pretty consistent.  \n",
    "# If we take the median of those cells, it should give us a good estimate of median under\n",
    "# No-signal conditions.\n",
    "row_mean = np.mean(detectArray, axis = 1)\n",
    "meanval = np.median(row_mean)\n",
    "sigma_sq = meanval / nCoherent\n",
    "medianval = np.median(detectArray)\n",
    "maxval = np.max(detectArray)\n",
    "#\n",
    "# We want pfa to reflect the possibility of an error in this measurement\n",
    "# there are len(detectArray) numbers we're going to evaluate\n",
    "pfa = 0.05 / len(detectArray)\n",
    "gamma = math.sqrt(2) * chi2.isf(pfa, 2*nCoherent)\n",
    "print (f' max = {maxval}\\n mean= {meanval}\\n sigma square = {sigma_sq}\\n  \\n pfa = {pfa} \\n gamma = {gamma}')\n",
    "threshold = sigma_sq * gamma\n",
    "print (f'Threshold = {threshold}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbb3ec4-6498-44ce-8d2d-eb3a8ff15be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = sv23_Results[\"freqtable\"]\n",
    "plotAcqSearch(101, settings, detectArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e7c385-321e-48bf-a312-703a5e5ba46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "nCoherent = 5\n",
    "nNonCoherent = 10\n",
    "pfa = 0.05\n",
    "sample_variance = mean / nCoherent\n",
    "erf = chi2.isf(pfa, 2*nCoherent)\n",
    "print (f'erf = {erf}')\n",
    "\n",
    "threshold = (gamma / nNonCoherent )* math.sqrt(sample_variance)\n",
    "print (f'gamma = {gamma}, \\nthreshold = {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84680899-da92-495c-965a-3fa168cc3ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = sv23_Results [\"signalDetected\"]\n",
    "mean = sv23_Results[\"mean\"]\n",
    "variance = sv23_Results[\"var\"]\n",
    "sdev = math.sqrt(variance)\n",
    "threshold = sv23_Results[\"threshold\"]\n",
    "maxVal = sv23_Results[\"maxValue\"]\n",
    "\n",
    "print (f'Signal Detected: {sig}')\n",
    "print (f'Peak Value = {maxVal}')\n",
    "print (f'Signal Threshold: {threshold}')\n",
    "print (f'Peak / Threshold: {maxVal / threshold}')\n",
    "print (f'Threshold / sdev: {threshold / sdev}')\n",
    "print (f'Mean Value: {mean:.2e}, standard deviation: {sdev:.2e}')\n",
    "print (f'peak / threshold ratio: {maxVal / threshold}')\n",
    "print (f'(peak - mean) / standard dev: {((maxVal - mean) / sdev)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa7c47a-476e-4432-9b10-ab61c961f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv23_matrix = sv23_Results[\"detector\"]\n",
    "plotAcqSearch(101, settings, sv23_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05936412-3c6a-4c78-8f1d-d7c18cb689c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(6)\n",
    "print(a)\n",
    "b = a.reshape((2,3))\n",
    "print (b)\n",
    "\n",
    "c = np.roll(b,shift=1, axis = 1)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0238d9c-2950-49e9-bfa7-f91e8fa87098",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "print (a)\n",
    "b = np.roll(a, 1)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf197c-b51b-48bc-a216-5ae5e825997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.fft as sfft\n",
    "\n",
    "x32 = np.arange(8, dtype=np.complex64)\n",
    "x64 = np.arange(8, dtype=np.complex128)\n",
    "\n",
    "print(\"NumPy FFT on complex64:\", np.fft.fft(x32).dtype)\n",
    "print(\"SciPy FFT on complex64:\", sfft.fft(x32).dtype)\n",
    "\n",
    "print(\"NumPy FFT on complex128:\", np.fft.fft(x64).dtype)\n",
    "print(\"SciPy FFT on complex128:\", sfft.fft(x64).dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2056fae-c4e7-44e8-b2c5-77aa3af52b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
